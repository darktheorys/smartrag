{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/burak/repos/smartrag\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/burak/repos/smartrag/.venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rsHJQRAAfFyp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        ")\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
        "from pprint import pp\n",
        "from utils import secrets\n",
        "from models import QueryAmbiguation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IqmP2iGPPkNn"
      },
      "outputs": [],
      "source": [
        "is_bool = False\n",
        "domain = \"MEDICAL\"\n",
        "dataset_name = \"medquad\"\n",
        "top_n = 10\n",
        "df = pd.read_csv(f\"{dataset_name}_ambiguous_with_top{top_n}_merged.csv\", index_col=0)\n",
        "df = df[df.valid == 1].reset_index()\n",
        "n_queries = len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msg4X4k6jJrv"
      },
      "source": [
        "### Chain Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oRZ_CJ68gDeF"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    api_key=secrets.get(\"OPENAI_API_KEY\"),\n",
        "    max_tokens=4096,\n",
        "    temperature=0.0,\n",
        "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
        ")\n",
        "embedder = OpenAIEmbeddings(api_key=secrets.get(\"OPENAI_API_KEY\"), model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "# model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\").to(\"cuda\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
        "# model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/xlm-roberta-base\").to(\"cuda\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\")\n",
        "# model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\").to(\"cuda\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "# model = AutoModelForMaskedLM.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"medicalai/ClinicalBERT\").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:08<00:00,  4.32s/it]\n",
            "100%|██████████| 11/11 [00:01<00:00,  5.64it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.14it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  3.93it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.11it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.88it/s]\n",
            "100%|██████████| 11/11 [00:04<00:00,  2.35it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.54it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.47it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  2.83it/s]\n",
            "100%|██████████| 10/10 [00:05<00:00,  1.80it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.19it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.22it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  2.94it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  2.80it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.55it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.36it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.31it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.06it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n",
            "100%|██████████| 8/8 [00:02<00:00,  3.12it/s]\n",
            "100%|██████████| 13/13 [00:03<00:00,  3.54it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.08it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
            "100%|██████████| 3/3 [00:01<00:00,  2.06it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.49it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.67it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.54it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.82it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.12it/s]\n",
            "100%|██████████| 12/12 [00:04<00:00,  2.61it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.30it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  2.79it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.05it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  7.14it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.33it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.46it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00, 10.43it/s]\n",
            "100%|██████████| 11/11 [00:21<00:00,  1.97s/it]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.15it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00,  3.54it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  7.09it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  7.92it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.17it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  8.15it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.20it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.92it/s]\n",
            "100%|██████████| 11/11 [00:00<00:00, 11.34it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
            "100%|██████████| 19/19 [00:03<00:00,  5.47it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.65it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.11it/s]\n",
            "100%|██████████| 11/11 [00:00<00:00, 11.04it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.72it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  9.00it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.89it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  5.93it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.30it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  7.19it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.87it/s]\n",
            "100%|██████████| 6/6 [00:01<00:00,  3.80it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
            "100%|██████████| 8/8 [00:02<00:00,  3.62it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  7.68it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  8.94it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.63it/s]\n",
            "100%|██████████| 14/14 [00:05<00:00,  2.78it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  5.59it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.30it/s]\n",
            "100%|██████████| 9/9 [00:02<00:00,  3.32it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.97it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  2.87it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.13it/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.28it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  5.65it/s]\n",
            "100%|██████████| 11/11 [00:04<00:00,  2.69it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.54it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.25it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.16it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.33it/s]\n",
            "100%|██████████| 8/8 [00:02<00:00,  3.99it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.46it/s]\n",
            "100%|██████████| 10/10 [00:03<00:00,  3.11it/s]\n",
            "100%|██████████| 15/15 [00:04<00:00,  3.02it/s]\n",
            "100%|██████████| 9/9 [00:03<00:00,  2.66it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00, 10.94it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "100%|██████████| 3/3 [00:01<00:00,  2.47it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.37it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.48it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.77it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.30it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.19it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.34it/s]\n",
            "100%|██████████| 7/7 [00:01<00:00,  5.91it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.18it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.40it/s]\n",
            "100%|██████████| 13/13 [00:03<00:00,  3.75it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n",
            "100%|██████████| 11/11 [00:01<00:00,  6.57it/s]\n",
            "100%|██████████| 12/12 [00:02<00:00,  4.59it/s]\n",
            "100%|██████████| 8/8 [00:02<00:00,  3.52it/s]\n",
            "100%|██████████| 7/7 [00:02<00:00,  3.34it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.77it/s]\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.95it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.85it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  5.14it/s]\n",
            "100%|██████████| 11/11 [00:02<00:00,  4.29it/s]\n",
            "100%|██████████| 11/11 [00:03<00:00,  2.99it/s]\n"
          ]
        }
      ],
      "source": [
        "for query_n in range(n_queries):\n",
        "    if df.loc[query_n, \"valid\"] != 1:\n",
        "        continue\n",
        "\n",
        "    example = df.loc[query_n, \"ambiguous_question\"]\n",
        "    ambiguities = json.loads(df.loc[query_n, \"possible_ambiguities\"])\n",
        "    ambiguities = QueryAmbiguation(**ambiguities)\n",
        "\n",
        "    # focus on only the first ambiguity in a query\n",
        "    amb = ambiguities.full_form_abbrv_map[0]\n",
        "    to_be_masked_example = example.replace(amb.abbreviation, amb.abbreviation + \" ({abbreviation})\")\n",
        "\n",
        "    possibilities = [amb.full_form.casefold().strip()]\n",
        "    if not pd.isna(df.loc[query_n, f\"top_{top_n}_full_form\"]):\n",
        "        possibilities += list(\n",
        "            map(lambda x: x.casefold().strip(), df.loc[query_n, f\"top_{top_n}_full_form\"].split(\"<-->\")[0].split(\"<->\"))\n",
        "        )\n",
        "    # add llm suggestion to df\n",
        "    # possibilities += [df.loc[query_n, \"llm_full_form_suggestion\"]]\n",
        "\n",
        "    possibility_probs = []\n",
        "    for pos in tqdm(possibilities):\n",
        "        tokenized = tokenizer(pos, return_tensors=\"pt\")\n",
        "        length = len(tokenized.input_ids[0]) - 2\n",
        "        prev = [tokenizer.mask_token_id for _ in range(length)]\n",
        "        logit_sum = 0.0\n",
        "        for i in range(length):\n",
        "            current_token_id = tokenized.input_ids[0][i + 1]\n",
        "\n",
        "            masked_example = to_be_masked_example.format(abbreviation=tokenizer.decode(prev))\n",
        "            tokenized_example = tokenizer(masked_example, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "            mask_start = tokenized_example.input_ids[0].tolist().index(tokenizer.mask_token_id)\n",
        "            with torch.no_grad():\n",
        "                filled_logits = torch.log_softmax(model(**tokenized_example).logits[0, mask_start, :], dim=0)\n",
        "            current_logit = filled_logits[current_token_id]\n",
        "\n",
        "            prev[i] = current_token_id\n",
        "            logit_sum += current_logit\n",
        "            # print(\"Expected Token:\", tokenizer.decode([tokenized.input_ids[0][i+1]]),\n",
        "            #      \"\\n\\tProbability:\",torch.exp(current_logit).item(),\n",
        "            #      \"\\n\\tBest token:\", tokenizer.decode(filled_logits.argmax(dim=1)),\n",
        "            #      \"\\n\\t\\tProbability:\", torch.exp(filled_logits.max(dim=1)[0]).item())\n",
        "        possibility_probs.append(torch.exp(logit_sum).item())\n",
        "\n",
        "    df.loc[query_n, \"ground_truth_full_form_prob\"] = possibility_probs.pop(0)\n",
        "    possibilities.pop(0)\n",
        "    if possibilities:\n",
        "        arr = np.asarray(possibility_probs)\n",
        "        most_likely_index = arr.argmax()\n",
        "        df.loc[query_n, \"most_likely_full_form\"] = possibilities[most_likely_index]\n",
        "        df.loc[query_n, \"most_likely_full_form_prob\"] = arr.max()\n",
        "        df.loc[query_n, f\"top_{top_n}_full_form_probs\"] = \"/\".join(map(str, possibility_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(f\"{dataset_name}_ambiguous_with_top{top_n}_merged_MLM_clinicalbert.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/123 [00:05<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     possibilities \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[query_n, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_full_form\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<-->\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<->\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m candidate_queries \u001b[38;5;241m=\u001b[39m [example] \u001b[38;5;241m+\u001b[39m [masked_example\u001b[38;5;241m.\u001b[39mformat(abbreviation\u001b[38;5;241m=\u001b[39mpos) \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m possibilities]\n\u001b[0;32m---> 19\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_queries\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m similarities \u001b[38;5;241m=\u001b[39m (embeddings[:\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m@\u001b[39m embeddings[\u001b[38;5;241m1\u001b[39m:, :]\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     23\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[query_n, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth_full_form_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m similarities[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:517\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 517\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:333\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    331\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 333\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    337\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py:113\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    107\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/openai/_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[0;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/openai/_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/openai/_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/openai/_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/openai/_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/openai/_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/repos/smartrag/.venv/lib/python3.12/site-packages/openai/_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    996\u001b[0m )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "for query_n in tqdm(range(n_queries)):\n",
        "    if df.loc[query_n, \"valid\"] != 1:\n",
        "        continue\n",
        "\n",
        "    example = df.loc[query_n, \"ambiguous_question\"]\n",
        "    ambiguities = json.loads(df.loc[query_n, \"possible_ambiguities\"])\n",
        "    ambiguities = QueryAmbiguation(**ambiguities)\n",
        "\n",
        "    # focus on only the first ambiguity\n",
        "    amb = ambiguities.full_form_abbrv_map[0]\n",
        "    masked_example = example.replace(amb.abbreviation, amb.abbreviation + \" ({abbreviation})\")\n",
        "\n",
        "    possibilities = [amb.full_form]\n",
        "    if not pd.isna(df.loc[query_n, f\"top_{top_n}_full_form\"]):\n",
        "        possibilities += df.loc[query_n, f\"top_{top_n}_full_form\"].split(\"<-->\")[0].split(\"<->\")\n",
        "\n",
        "    candidate_queries = [example] + [masked_example.format(abbreviation=pos) for pos in possibilities]\n",
        "\n",
        "    embeddings = np.asarray(embedder.embed_documents(candidate_queries))\n",
        "\n",
        "    similarities = (embeddings[:1, :] @ embeddings[1:, :].T).flatten()\n",
        "\n",
        "    df.loc[query_n, \"ground_truth_full_form_prob\"] = similarities[0]\n",
        "    possibilities.pop(0)\n",
        "\n",
        "    similarities = similarities[1:]\n",
        "    if possibilities:\n",
        "        best_candidate = similarities.argmax()\n",
        "\n",
        "        df.loc[query_n, \"most_likely_full_form\"] = possibilities[best_candidate]\n",
        "        df.loc[query_n, \"most_likely_full_form_prob\"] = similarities[best_candidate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(f\"{dataset_name}_ambiguous_with_top{top_n}_merged_EMBED_text-embedding-3-large.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models import Selector\n",
        "\n",
        "selector_output_parser = PydanticOutputParser(pydantic_object=Selector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selector_system_prompt = \"\"\"User will give you a query, in this query there will be an abbreviation. Your task is to resolve that abbreviation.\n",
        "User will also provide possible full-forms that you can select from. Please do best you can while selecting from the given list of options.\n",
        "If you cant find and appropriate selection from given options, please use selection_id as -1.\n",
        "\n",
        "Query domain will be {domain}.\n",
        "\n",
        "Format Instructions:\n",
        "{format_instructions}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selector_user_message = \"\"\"Query: {query}\n",
        "Abbreviation: {abbrv}\n",
        "Options:\n",
        "{options}\n",
        "Selection:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selector_messages = [\n",
        "    SystemMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=selector_system_prompt,\n",
        "            input_variables=[],\n",
        "            partial_variables={\n",
        "                \"format_instructions\": selector_output_parser.get_format_instructions(),\n",
        "                \"domain\": domain,\n",
        "            },\n",
        "        )\n",
        "    ),\n",
        "    HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(template=selector_user_message, input_variables=[\"query\", \"abbrv\", \"options\"])\n",
        "    ),\n",
        "]\n",
        "selector_prompt = ChatPromptTemplate.from_messages(messages=selector_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selector_chain = selector_prompt | llm | selector_output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for query_n in tqdm(range(n_queries)):\n",
        "    if df.loc[query_n, \"valid\"] != 1:\n",
        "        continue\n",
        "\n",
        "    query = df.loc[query_n, \"ambiguous_question\"]\n",
        "    ambiguities = json.loads(df.loc[query_n, \"possible_ambiguities\"])\n",
        "    ambiguities = QueryAmbiguation(**ambiguities)\n",
        "\n",
        "    # focus on only the first ambiguity\n",
        "    amb = ambiguities.full_form_abbrv_map[0]\n",
        "\n",
        "    possibilities = []\n",
        "    if not pd.isna(df.loc[query_n, f\"top_{top_n}_full_form\"]):\n",
        "        possibilities += df.loc[query_n, f\"top_{top_n}_full_form\"].split(\"<-->\")[0].split(\"<->\")\n",
        "\n",
        "    if not possibilities:\n",
        "        possibilities += [df.loc[query_n, \"llm_full_form_suggestion\"]]\n",
        "        df.loc[query_n, \"most_likely_full_form\"] = df.loc[query_n, \"llm_full_form_suggestion\"]\n",
        "    else:\n",
        "        resp = selector_chain.invoke(\n",
        "            {\n",
        "                \"abbrv\": amb.abbreviation,\n",
        "                \"query\": query,\n",
        "                \"options\": [f\"{i} - {opt}\\n\" for i, opt in enumerate(possibilities)],\n",
        "            }\n",
        "        )\n",
        "        df.loc[query_n, \"most_likely_full_form\"] = (\n",
        "            possibilities[resp.selection_id] if resp.selection_id != -1 else df.loc[query_n, \"llm_full_form_suggestion\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(f\"{dataset_name}_ambiguous_with_top{top_n}_merged_LLM_selector.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Boolean or STR Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models import AnswerBool, AnswerStr, QueryAmbiguation\n",
        "\n",
        "output_parser_bool = PydanticOutputParser(pydantic_object=AnswerBool)\n",
        "output_parser_str = PydanticOutputParser(pydantic_object=AnswerStr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_parser_answer = output_parser_str if not is_bool else output_parser_bool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sys_message_answer = \"\"\"Answer the given questions as concise and short as possible. Do not output something else.\n",
        "Additionally, an intent and requirements for the answer can be provided by the user. Take them into consideration while answering.\n",
        "Domain of the questions is {domain}.\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "user_message_answer = \"\"\"Query:{query}\n",
        "Intent:{intent}\n",
        "Requirements:{reqs}\n",
        "Output:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages_answer = [\n",
        "    SystemMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=sys_message_answer,\n",
        "            input_variables=[],\n",
        "            partial_variables={\"format_instructions\": output_parser_answer.get_format_instructions(), \"domain\": domain},\n",
        "        )\n",
        "    ),\n",
        "    HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=user_message_answer, input_variables=[\"query\", \"intent\", \"reqs\"], partial_variables={}\n",
        "        )\n",
        "    ),\n",
        "]\n",
        "prompt_answer = ChatPromptTemplate.from_messages(messages=messages_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain_answer = prompt_answer | llm | output_parser_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEp4qtE9N_s7"
      },
      "source": [
        "## LLM Answer Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models import AnswerJudge\n",
        "\n",
        "output_parser_score = PydanticOutputParser(pydantic_object=AnswerJudge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sys_message_score = \"\"\"Given a query and two candidate answers and one real answer, your task is to compare two candidate answers.\n",
        "\n",
        "If first candidate is the only one that is similar to the actual answer you will output number '1'.\n",
        "If second candidate is the only one that is similar to the actual answer you will output number '2'.\n",
        "If both of the candidates show enough similarity to the real answer you will output number '3'.\n",
        "If none of the candidates show enough similarity to the real answer you will output number '4'.\n",
        "\n",
        "Format instructions:\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "user_message_score = \"\"\"Real Answer: {answer}\n",
        "First Candidate: {amb}\n",
        "Second Candidate: {unamb}\n",
        "Output:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages_score = [\n",
        "    SystemMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=sys_message_score,\n",
        "            input_variables=[],\n",
        "            partial_variables={\"format_instructions\": output_parser_score.get_format_instructions()},\n",
        "        )\n",
        "    ),\n",
        "    HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=user_message_score, input_variables=[\"answer\", \"amb\", \"unamb\"], partial_variables={}\n",
        "        )\n",
        "    ),\n",
        "]\n",
        "prompt_score = ChatPromptTemplate.from_messages(messages=messages_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain_score = prompt_score | llm | output_parser_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqfXn0LyPZlG"
      },
      "source": [
        "## Accuracy Calc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUl0IKjdyQsr"
      },
      "outputs": [],
      "source": [
        "correct = [set(), set()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "l5bogdRtWwTD",
        "outputId": "426280a3-5f9f-4082-ba7b-081f71ad95a6"
      },
      "outputs": [],
      "source": [
        "lm1 = RunnableLambda(lambda x: chain_answer.invoke(x[\"amb\"]))\n",
        "lm2 = RunnableLambda(lambda x: chain_answer.invoke(x[\"unamb\"]))\n",
        "chain_ = RunnableParallel(amb=lm1, unamb=lm2)\n",
        "\n",
        "for df_index in tqdm(range(n_queries)):\n",
        "    if df.loc[df_index, \"valid\"] != 1:\n",
        "        continue\n",
        "\n",
        "    ambiguities = QueryAmbiguation(**json.loads(df.loc[df_index, \"possible_ambiguities\"]))\n",
        "    amb = ambiguities.full_form_abbrv_map[0]\n",
        "    amb_question = df.loc[df_index, \"ambiguous_question\"]\n",
        "    unamb_question = df.loc[df_index, \"unambiguous_question\"]\n",
        "    if df.loc[df_index, \"most_likely_full_form\"]:\n",
        "        amb_question = amb_question.replace(\n",
        "            amb.abbreviation, amb.abbreviation + f\" ({df.loc[df_index, 'most_likely_full_form']})\"\n",
        "        )\n",
        "    # amb_question = amb_question.replace(amb.abbreviation, amb.abbreviation + f\" ({df.loc[df_index, 'llm_full_form_suggestion']})\")\n",
        "\n",
        "    df.loc[df_index, \"disambiguated_question\"] = amb_question\n",
        "\n",
        "    if \"answer\" in df and is_bool:\n",
        "        answer = [df.loc[df_index, \"answer\"] in [\"TRUE\", \"True\", True, 1, \"1\"]]\n",
        "    elif \"answer\" in df:\n",
        "        answer = [df.loc[df_index, \"answer\"]]\n",
        "    elif \"answers\" in df:\n",
        "        answer = eval(df.loc[df_index, \"answers\"])[\"text\"]\n",
        "    else:\n",
        "        answer = []\n",
        "        for i in eval(df.loc[df_index, \"annotations\"]):\n",
        "            if i[\"type\"] == \"singleAnswer\":\n",
        "                answer.extend(list(map(lambda x: x.casefold(), i[\"answer\"])))\n",
        "            elif i[\"type\"] == \"multipleQAs\":\n",
        "                for pair in i[\"qaPairs\"]:\n",
        "                    answer.extend(list(map(lambda x: x.casefold(), pair[\"answer\"])))\n",
        "            else:\n",
        "                raise RuntimeError(i)\n",
        "\n",
        "    response = chain_.invoke(\n",
        "        {\n",
        "            \"amb\": {\n",
        "                \"query\": amb_question,\n",
        "                \"intent\": df.loc[df_index, \"intent\"],\n",
        "                \"reqs\": \"\\n\".join(eval(df.loc[df_index, \"requirements\"])),\n",
        "            },\n",
        "            \"unamb\": {\n",
        "                \"query\": unamb_question,\n",
        "                \"intent\": df.loc[df_index, \"intent\"],\n",
        "                \"reqs\": \"\\n\".join(eval(df.loc[df_index, \"requirements\"])),\n",
        "            },\n",
        "        }\n",
        "    )\n",
        "    if is_bool:\n",
        "        amb_correct = response[\"amb\"].answer == answer[0]\n",
        "        unamb_correct = response[\"unamb\"].answer == answer[0]\n",
        "    else:\n",
        "        response_ = chain_score.invoke(\n",
        "            {\"answer\": answer[0], \"amb\": response[\"amb\"].answer, \"unamb\": response[\"unamb\"].answer}\n",
        "        )\n",
        "\n",
        "        if response_.selection == 1:\n",
        "            amb_correct = True\n",
        "            unamb_correct = False\n",
        "        elif response_.selection == 2:\n",
        "            amb_correct = False\n",
        "            unamb_correct = True\n",
        "        elif response_.selection == 3:\n",
        "            amb_correct = True\n",
        "            unamb_correct = True\n",
        "        else:\n",
        "            amb_correct = False\n",
        "            unamb_correct = False\n",
        "    # amb_correct = any([response[\"amb\"].answer.casefold() in a.casefold() or a.casefold() in response[\"amb\"].answer.casefold() for a in answer])\n",
        "    # unamb_correct = any([response[\"unamb\"].answer.casefold() in a.casefold() or a.casefold() in response[\"unamb\"].answer.casefold() for a in answer])\n",
        "\n",
        "    df.loc[df_index, \"disambiguated_question_answered_by_gpt4\"] = response[\"amb\"].answer\n",
        "    df.loc[df_index, \"unambiguous_question_answered_by_gpt4\"] = response[\"unamb\"].answer\n",
        "\n",
        "    if amb_correct:\n",
        "        correct[0].add(df_index)\n",
        "        df.loc[i, \"disambiguated_question_answered_correct_by_gpt4\"] = True\n",
        "    if unamb_correct:\n",
        "        correct[1].add(df_index)\n",
        "        df.loc[i, \"unambiguous_question_answered_correct_by_gpt4\"] = True\n",
        "    df.loc[df_index, \"answers\"] = str(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdEnnVAKX9QD",
        "outputId": "943b438b-3211-496d-9392-f9669db39c5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.6476190476190476, 0.8285714285714286)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(correct[0]) / len(df), len(correct[1]) / len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkf-1gvfyHBC"
      },
      "outputs": [],
      "source": [
        "df.to_csv(f\"{dataset_name}_ambiguous_with_top{top_n}_merged_with_intent_compared.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bb336d4b807487b8166df1ce30148f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8423a19a8da24aeb8754ab15b46bca78",
            "placeholder": "​",
            "style": "IPY_MODEL_1e45f7ab3ba64707a8cc7afe4bc58450",
            "value": "Generating train split: "
          }
        },
        "13b1d5d0d2574352b61d5e0981868296": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_615d4cb2b713432da512b8873ab378f6",
            "placeholder": "​",
            "style": "IPY_MODEL_1671c6ee7d324cdeb833161785781ad1",
            "value": "Downloading data: 100%"
          }
        },
        "1671c6ee7d324cdeb833161785781ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1941809356964d50b72cf2ddcc76220e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e45f7ab3ba64707a8cc7afe4bc58450": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e689779426c445d8d4009bb3a547199": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "27205c9553a14ef99bbb620c8bdb4eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a605f173cb34d57a8bcea1386b5872e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7f231acf314e9d81706f4a99addc90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3602c1ca00334b94ac7c5a3b3b2e508c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8de532d1bb04f84a882e6d8af6144ee",
            "max": 233,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf71d6c81e424e3f8eec85a74381e5a6",
            "value": 233
          }
        },
        "360dedd784364c57901855cd80b7cd9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e58b9bcfe934577aba6f797adb81268": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b272be4ba0664d379ac7f3f4c517ad12",
            "placeholder": "​",
            "style": "IPY_MODEL_7679d02c5f4b45c7b5f8e5813540f078",
            "value": "Downloading readme: 100%"
          }
        },
        "42f8f91e530c4d2694d5e5508a0d7749": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e689779426c445d8d4009bb3a547199",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1bd17882e4c433f83953dbdb40638e8",
            "value": 1
          }
        },
        "49c126698da94e7db48d9566449f0b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b18f27f146254d6ea2df3ab51cecfe03",
            "placeholder": "​",
            "style": "IPY_MODEL_2d7f231acf314e9d81706f4a99addc90",
            "value": " 22.5M/22.5M [00:01&lt;00:00, 14.7MB/s]"
          }
        },
        "53726a8b02f046168cc2e48c7fcca63a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a5dce6e60734395b1e6d1c12024c21c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "615d4cb2b713432da512b8873ab378f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61eb1ea3f5df4c739b0b78a9efad4eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfe4137d4e5d446fa81f9663ba48122c",
            "placeholder": "​",
            "style": "IPY_MODEL_360dedd784364c57901855cd80b7cd9b",
            "value": " 233/233 [00:00&lt;00:00, 2.56kB/s]"
          }
        },
        "6faff60b90494c73b8a5afc8ecc3f3eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b47711da1a3d4f72ba3b60dd6aafb7ef",
            "placeholder": "​",
            "style": "IPY_MODEL_27205c9553a14ef99bbb620c8bdb4eba",
            "value": " 16407/0 [00:01&lt;00:00, 15129.90 examples/s]"
          }
        },
        "7679d02c5f4b45c7b5f8e5813540f078": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8423a19a8da24aeb8754ab15b46bca78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac333667268640faab73d3f4b3f38a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bb336d4b807487b8166df1ce30148f9",
              "IPY_MODEL_42f8f91e530c4d2694d5e5508a0d7749",
              "IPY_MODEL_6faff60b90494c73b8a5afc8ecc3f3eb"
            ],
            "layout": "IPY_MODEL_5a5dce6e60734395b1e6d1c12024c21c"
          }
        },
        "b18f27f146254d6ea2df3ab51cecfe03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b272be4ba0664d379ac7f3f4c517ad12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b47711da1a3d4f72ba3b60dd6aafb7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfe4137d4e5d446fa81f9663ba48122c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1bd17882e4c433f83953dbdb40638e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c654aaae392b47ce81c2074bf9b5e571": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf71d6c81e424e3f8eec85a74381e5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0ddf607fc14405096bd318401000526": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e58b9bcfe934577aba6f797adb81268",
              "IPY_MODEL_3602c1ca00334b94ac7c5a3b3b2e508c",
              "IPY_MODEL_61eb1ea3f5df4c739b0b78a9efad4eaf"
            ],
            "layout": "IPY_MODEL_2a605f173cb34d57a8bcea1386b5872e"
          }
        },
        "eb72d03357584247943038998686fa40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13b1d5d0d2574352b61d5e0981868296",
              "IPY_MODEL_ed657d49d5794360b641cab48b243d21",
              "IPY_MODEL_49c126698da94e7db48d9566449f0b85"
            ],
            "layout": "IPY_MODEL_c654aaae392b47ce81c2074bf9b5e571"
          }
        },
        "ed657d49d5794360b641cab48b243d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53726a8b02f046168cc2e48c7fcca63a",
            "max": 22466890,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1941809356964d50b72cf2ddcc76220e",
            "value": 22466890
          }
        },
        "f8de532d1bb04f84a882e6d8af6144ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
